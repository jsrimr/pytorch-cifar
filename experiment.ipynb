{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "from models.mobilenetv2 import *\n",
    "from net_transform import proj_wider_cout_expansion_wider_cin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "net = MobileNetV2()\n",
    "net = net.to(device)\n",
    "if device == 'cuda':\n",
    "    net = torch.nn.DataParallel(net)\n",
    "    cudnn.benchmark = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load('./checkpoint/ckpt.pth')\n",
    "net.load_state_dict(checkpoint['net'])\n",
    "best_acc = checkpoint['acc']\n",
    "start_epoch = checkpoint['epoch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.mobilenetv2_wider import MobileNetV2_wider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_wider = MobileNetV2_wider()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bn1 = net.module.layers[12].bn1.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bn1.state_dict()['weight'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bn = torch.nn.BatchNorm2d(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in bn.state_dict().items():\n",
    "    print(k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_ = torch.nn.Conv2d(5, 10,3)\n",
    "conv_.state_dict().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(net.module.layers[12].conv1.state_dict().values())[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(net.module.layers[12].conv3.state_dict().values())[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(net_wider.layers[13].conv3.state_dict().values())[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight 를 모아서 한번에 이식해야 함. c_in 만 늘리는 경우가 있기 때문에 그 때그때 넣어줄 수 가 없음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_wider.layers[12].conv3.state_dict()['weight'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for dict_ in net.module.layers[block_num].bn3.state_dict().items():\n",
    "    print(dict_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from net_transform import proj_wider_cout_expansion_wider_cin\n",
    "parent_net = net.to(\"cpu\")\n",
    "child_net = net_wider\n",
    "block_num = 12\n",
    "\n",
    "child_l3_weights, child_bn3, child_l4_weights = [child_net.layers[block_num].conv3.state_dict(), child_net.layers[block_num].bn3.state_dict(), child_net.layers[block_num+1].conv1.state_dict()]\n",
    "l3_weights, bn3, l4_weights = [parent_net.module.layers[block_num].conv3.state_dict(), parent_net.module.layers[block_num].bn3.state_dict(), parent_net.module.layers[block_num+1].conv1.state_dict()]\n",
    "parent_weights = [child_l3_weights, child_bn3, child_l4_weights]\n",
    "child_weights = [l3_weights, bn3, l4_weights]\n",
    "\n",
    "new_weight_list = proj_wider_cout_expansion_wider_cin(child_weights, parent_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "child_net.layers[block_num].conv3.data = (new_weight_list[0])\n",
    "child_net.layers[block_num].bn3.data = (new_weight_list[1])\n",
    "child_net.layers[block_num+1].conv1.data = (new_weight_list[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_net.module.layers[block_num].conv3.state_dict()['weight'].numpy().shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(1,576,32,32)\n",
    "parent_result = parent_net.module.layers[block_num].conv3.forward(x)\n",
    "parent_result = parent_net.module.layers[block_num].bn3.forward(parent_result)\n",
    "parent_result = parent_net.module.layers[block_num+1].conv1.forward(parent_result)\n",
    "parent_result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "child_result = child_net.layers[block_num].conv3.forward(x)\n",
    "child_result = child_net.layers[block_num].bn3.forward(child_result)\n",
    "child_result = child_net.layers[block_num+1].conv1.forward(child_result)\n",
    "child_result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_net.module.layers[block_num].conv3.forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "child_net.layers[block_num].bn3.state_dict()['weight'].numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.module.layers[block_num].bn3.state_dict()['weight'].numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 ~ 12번쨰 블록을 늘림. layers[5]\n",
    "stage_block_weights = []\n",
    "block_list = range(10,13)\n",
    "child_net = net_wider\n",
    "parent_net = net\n",
    "\n",
    "for block_num in block_list:\n",
    "    stage_block_weights = []\n",
    "\n",
    "    if block != 0 : # 첫번째가 아니면\n",
    "        parent_block_weights = parent_net[]\n",
    "        child_block_weights = \n",
    "        parent_next_block_weight = \n",
    "        child_next_block_weight = \n",
    "        new_block_weight = \n",
    "\n",
    "    # proj c_out, bn3 ~ next exp c_in늘림\n",
    "    child_proj_weights = child_net.layers[block_num].conv3.state_dict()['weight'].numpy()\n",
    "    child_bn3_weights = child_net.layers[block_num].conv3.state_dict()['weight'].numpy()\n",
    "                                 \n",
    "    parent_proj_weights = parent_net.module.layers[block_num].conv3.state_dict()['weight'].numpy()\n",
    "    parent_bn3_weights = parent_net.module.layers[block_num].conv3.state_dict()['weight'].numpy()\n",
    "\n",
    "    if flag:  # 위의 로직이 실행되어 proj c_in 이 늘어난 경우를 처리\n",
    "        parent_proj_and_bn3_weights[0] = new_block_weights[-2]\n",
    "        \n",
    "    child_next_expansion_weight = child_net.layers[block_num].conv3.state_dict()['weight'].numpy()\n",
    "    parent_next_expansion_weight = parent_net.module.layers[block_num].conv3.state_dict()['weight'].numpy()\n",
    "\n",
    "    new_weights = proj_wider_cout_expansion_wider_cin(child_proj_and_bn3_weights,\n",
    "                                                        parent_proj_and_bn3_weights,\n",
    "                                                        child_next_expansion_weight,\n",
    "                                                        parent_next_expansion_weight)\n",
    "    if new_block_weights:\n",
    "        new_block_weights[-2:] = new_weights[:2]\n",
    "    else:\n",
    "        new_block_weights = new_weights[:2]\n",
    "\n",
    "    stage_block_weights.extend(new_block_weights)\n",
    "\n",
    "set_block_weights(child_cc, stage_block_weights, stage=True, parent=False, filtered_keys=filtered_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wider_MBconv_block(block_weights, child_block_weights, next_block_weight=None, child_next_block_weights=None,\n",
    "                       use_SE=True, next_stage_first_block=False):\n",
    "    \"\"\"\n",
    "    x -> older_conv_block -> x'\n",
    "    x -> wider_conv_block -> x' + 0.1x'\n",
    "    returns enlarged weight of blocks and next_block's expansion layer\n",
    "    : [expansion, bn1, depthwise, b2, [se1, se_bias1, se2, se_bias2], proj, next_block_expansion], [b1, b2, 'se', b3]\n",
    "    \"\"\"\n",
    "\n",
    "    # ex) l1_weights = [{k:List[np.array()]} * 4]\n",
    "    keys, [l1_weights, bn1, l2_weights, bn2, se1_weights, se1_bias, se2_weights, se2_bias, l3_weights,\n",
    "           bn3] = decompose_key_val(block_weights)\n",
    "\n",
    "    child_keys, [child_l1_weights, child_bn1, child_l2_weights, child_bn2, child_se1_weights, child_se1_bias,\n",
    "                 child_se2_weights, child_se2_bias, child_l3_weights,\n",
    "                 child_bn3] = decompose_key_val(child_block_weights)\n",
    "\n",
    "    new_weights = []\n",
    "\n",
    "    # expansion\n",
    "    # new_width = int(width_coeff * l1_weights[0].shape[3])\n",
    "    new_width = child_l1_weights[0].shape[-1]\n",
    "    rand = np.random.randint(l1_weights[0].shape[-1], size=(new_width - l1_weights[0].shape[-1]))\n",
    "    replication_factor = np.bincount(rand)\n",
    "    factor = replication_factor[rand] + 1\n",
    "\n",
    "    student_w1 = np.array(deepcopy(l1_weights))\n",
    "    student_bn1 = np.array(deepcopy(bn1))\n",
    "    student_w2 = np.array(deepcopy(l2_weights))\n",
    "    student_bn2 = np.array(deepcopy(bn2))\n",
    "\n",
    "    if use_SE:\n",
    "        student_se1 = np.array(deepcopy(se1_weights))\n",
    "        student_se_bias1 = np.array(deepcopy(se1_bias))\n",
    "\n",
    "        student_se2 = np.array(deepcopy(se2_weights))\n",
    "        student_se_bias2 = np.array(deepcopy(se2_bias))\n",
    "\n",
    "    student_w3 = np.array(deepcopy(l3_weights))\n",
    "    student_bn3 = np.array(deepcopy(bn3))\n",
    "\n",
    "    # Expansion layer c_out update\n",
    "    # ex) [(1, 1, 24, 72)*B] + (4, 1, 1, 24, 7)\n",
    "    new_weight = np.array(student_w1)[:, :, :, :, rand]  #\n",
    "    student_w1 = np.concatenate((student_w1, new_weight), axis=-1)\n",
    "\n",
    "    # BN1 update\n",
    "    # ex) [(72,)*B] + (12,7)\n",
    "    new_weight = np.array(student_bn1)[:, rand]\n",
    "    student_bn1 = np.concatenate((student_bn1, new_weight), axis=-1)\n",
    "\n",
    "    # se2 c_out update\n",
    "    if use_SE:\n",
    "        new_weight = np.array(student_se2)[:, :, :, :, rand]\n",
    "        student_se2 = np.concatenate((student_se2, new_weight), axis=-1)\n",
    "        student_se_bias2 = np.concatenate((student_se_bias2, np.array(se2_bias)[:, rand]), axis=-1)\n",
    "\n",
    "    # c_in update : depthwise & proj, se1 if used SE\n",
    "    new_weight = np.array(student_w2)[:, :, :, rand, :]  # depthwise 는 normalize 필요없\n",
    "    student_w2 = np.concatenate((student_w2, new_weight), axis=-2)\n",
    "\n",
    "    # bn2\n",
    "    new_weight = np.array(student_bn2)[:, rand]\n",
    "    student_bn2 = np.concatenate((student_bn2, new_weight), axis=-1)\n",
    "\n",
    "    # proj c_in update\n",
    "    new_weight = np.array(student_w3)[:, :, :, rand, :] / factor.reshape(-1, 1)\n",
    "    student_w3 = np.concatenate((student_w3, new_weight), axis=-2)\n",
    "    student_w3[:, :, :, rand, :] = new_weight\n",
    "\n",
    "    # se1 c_in update\n",
    "    if use_SE:\n",
    "        new_weight = np.array(student_se1)[:, :, :, rand, :] / factor.reshape(-1, 1)\n",
    "        student_se1 = np.concatenate((student_se1, new_weight), axis=-2)\n",
    "        student_se1[:, :, :, rand, :] = new_weight\n",
    "\n",
    "    # se1 c_out, se2 c_in update\n",
    "    if use_SE:\n",
    "        # 출력부분을 변경하면 bias 도 바꾼다\n",
    "        new_width = child_se1_weights[0].shape[-1]\n",
    "        rand = np.random.randint(se1_weights[0].shape[-1], size=(new_width - se1_weights[0].shape[-1]))\n",
    "        replication_factor = np.bincount(rand)\n",
    "\n",
    "        # se1 c_out update\n",
    "        new_weight = np.array(student_se1)[:, :, :, :, rand]\n",
    "        student_se1 = np.concatenate((student_se1, new_weight), axis=-1)\n",
    "        student_se_bias1 = np.concatenate((student_se_bias1, np.array(se1_bias)[:, rand]), axis=-1)\n",
    "\n",
    "        # se2 c_in update\n",
    "        factor = replication_factor[rand] + 1\n",
    "\n",
    "        new_weight = np.array(student_se2)[:, :, :, rand, :] / factor.reshape(-1, 1)\n",
    "        student_se2 = np.concatenate((student_se2, new_weight), axis=-2)\n",
    "        student_se2[:, :, :, rand, :] = new_weight\n",
    "        # student_se_bias2 = np.concatenate((student_se_bias2, np.array(se2_bias)[:, rand]), axis=-1) !! c_in update 할 때는 bias 를 업데이트할 필요가 없다!\n",
    "\n",
    "    # add changed weight to result_dict : first_conv\n",
    "    new_weights.append(student_w1.astype('f'))\n",
    "    new_weights.append(student_bn1.astype('f'))\n",
    "\n",
    "    # add changed weight to result_dict : depthwise\n",
    "    new_weights.append(student_w2.astype('f'))\n",
    "    new_weights.append(student_bn2.astype('f'))\n",
    "\n",
    "    # SE\n",
    "    if use_SE:\n",
    "        new_weights.append(student_se1.astype('f'))\n",
    "        new_weights.append(student_se_bias1.astype('f'))\n",
    "        new_weights.append(student_se2.astype('f'))\n",
    "        new_weights.append(student_se_bias2.astype('f'))\n",
    "\n",
    "    if next_stage_first_block:\n",
    "        # proj c_out update\n",
    "        new_width = child_l3_weights[0].shape[-1]\n",
    "        rand_proj = np.random.randint(l3_weights[0].shape[-1], size=(new_width - l3_weights[0].shape[-1]))\n",
    "\n",
    "        new_weight = student_w3[:, :, :, :, rand_proj]\n",
    "        new_weight *= 1.005\n",
    "        # new_weight = np.random.randn(*new_weight.shape)\n",
    "\n",
    "        student_w3 = np.concatenate((student_w3, new_weight), axis=-1)\n",
    "\n",
    "        bn3_new_weight = np.array(bn3)[:, rand_proj]\n",
    "\n",
    "        # bn3_new_weight[0:4, :] = np.zeros_like(bn3_new_weight[0:4, :])  # beta\n",
    "        # bn3_new_weight[4:8, :] = np.ones_like(bn3_new_weight[4:8, :])  # gamma\n",
    "        # bn3_new_weight[(8, 9), :] = new_weight.mean(axis=1, keepdims=True)  # moving mean, exp_mean\n",
    "        # bn3_new_weight[(10, 11), :] = new_weight.var(axis=1, keepdims=True)  # moving var, exp_var\n",
    "\n",
    "        student_bn3 = np.concatenate((student_bn3, bn3_new_weight), axis=-1)\n",
    "\n",
    "    new_weights.append(student_w3.astype('f'))\n",
    "    new_weights.append(student_bn3.astype('f'))\n",
    "\n",
    "    return process_key_weight_to_result(keys, new_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wider_MBConvBlock(parent_block_weights, child_block_weights):\n",
    "       new_width = child_block_weights.conv1.out_channels\n",
    "       rand = np.random.randint(parent_block_weights.conv1.out_channels, size=(new_width - parent_block_weights.conv1.out_channels))\n",
    "       replication_factor = np.bincount(rand)\n",
    "       factor = replication_factor[rand] + 1\n",
    "\n",
    "       # Expansion layer 늘리기\n",
    "       student_w1 = np.array(deepcopy(l1_weights))\n",
    "       student_bn1 = np.array(deepcopy(bn1))\n",
    "       student_w2 = np.array(deepcopy(l2_weights))\n",
    "       student_bn2 = np.array(deepcopy(bn2))\n",
    "       # Depthwise layer Normalize\n",
    "\n",
    "       return process_key_weight_to_result(keys, new_weights)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
